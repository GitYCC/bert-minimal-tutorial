{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chinese_sentiment_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMiTk2/tra0e9GTFfSt/kDY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH_E6svUQa_U"
      },
      "source": [
        "# Chinese Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szYZxfyBQFaC",
        "outputId": "36bca9c5-6cc7-4be6-fd21-9bc80fc4c357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/GitYCC/bert-minimal-tutorial.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert-minimal-tutorial'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 26 (delta 8), reused 18 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRUAy5cjQrm8",
        "outputId": "5e9bfbb1-eb40-462c-c54d-cd9dce5e6eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd bert-minimal-tutorial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bert-minimal-tutorial\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7UI7hz5QxJL",
        "outputId": "6ac5ead9-996d-4797-bf32-2666ea88bee0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 829kB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 53.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 50.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 727kB 45.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 28.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 56.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 54.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 57.0MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcxBSB_KQ36A"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from utils import RunningAverage\n",
        "\n",
        "MODEL_NAME = 'bert-base-chinese'\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tKT4nZARDS_"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr3kbaBjRN_r"
      },
      "source": [
        "df = pd.read_csv('data/chinese_sentiment_classification.csv')\n",
        "df = df.sample(frac=1).reset_index(drop=True)  # shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRND_YmJST2E",
        "outputId": "35a0a0d6-696d-491e-a11c-23eadb8146fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>性價比高，外觀不錯,空間很不錯夠寬敞。在當時這款車兩側還有防撞梁挺不錯的。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>有好多小毛病，都是些無關痛癢的</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>最滿意致悅的車身尺寸，這個兩廂車2708mm的軸距，185cm的車寬，不會顯的小氣，非常適合...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>動力是短版，不過畢竟是1.5自然吸氣的發動機，夠用</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>小三外觀真的沒話說，挺漂亮的，尤其前臉，那個大嘴很好看，開起來還是很順暢的，尤其在高速上，在...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69995</th>\n",
              "      <td>1</td>\n",
              "      <td>氙氣大燈給力！車外形沒得說！後備箱大大大</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69996</th>\n",
              "      <td>0</td>\n",
              "      <td>暫時沒有什麼不滿意的，自己挑選的車</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69997</th>\n",
              "      <td>0</td>\n",
              "      <td>作為一款中型suv後備箱空間不足，三個小行李箱幾乎塞滿。吝嗇的鹵素大燈，蠟燭光亮名符其實。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69998</th>\n",
              "      <td>0</td>\n",
              "      <td>門側有異響，是橡膠條的，希望時間長一點就能解決，4s的哥們是這樣說的，新車緣故</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69999</th>\n",
              "      <td>1</td>\n",
              "      <td>車內很安靜</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       label                                               text\n",
              "0          1              性價比高，外觀不錯,空間很不錯夠寬敞。在當時這款車兩側還有防撞梁挺不錯的。\n",
              "1          0                                    有好多小毛病，都是些無關痛癢的\n",
              "2          1  最滿意致悅的車身尺寸，這個兩廂車2708mm的軸距，185cm的車寬，不會顯的小氣，非常適合...\n",
              "3          0                          動力是短版，不過畢竟是1.5自然吸氣的發動機，夠用\n",
              "4          1  小三外觀真的沒話說，挺漂亮的，尤其前臉，那個大嘴很好看，開起來還是很順暢的，尤其在高速上，在...\n",
              "...      ...                                                ...\n",
              "69995      1                               氙氣大燈給力！車外形沒得說！後備箱大大大\n",
              "69996      0                                  暫時沒有什麼不滿意的，自己挑選的車\n",
              "69997      0      作為一款中型suv後備箱空間不足，三個小行李箱幾乎塞滿。吝嗇的鹵素大燈，蠟燭光亮名符其實。\n",
              "69998      0            門側有異響，是橡膠條的，希望時間長一點就能解決，4s的哥們是這樣說的，新車緣故\n",
              "69999      1                                              車內很安靜\n",
              "\n",
              "[70000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDwgs9MgSWO2"
      },
      "source": [
        "class MultiClassDataset(Dataset):\n",
        "    def __init__(self, tokenizer, df, max_len=512, for_train=True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.for_train = for_train\n",
        "\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "        for _, row in df.iterrows():\n",
        "            self.texts.append(row['text'])\n",
        "            if for_train:\n",
        "                self.labels.append(row['label'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = tokens[:self.max_len-2]\n",
        "        processed_tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "\n",
        "        input_ids = torch.tensor(self.tokenizer.convert_tokens_to_ids(processed_tokens))\n",
        "        token_type_ids = torch.tensor([0] * len(processed_tokens))\n",
        "        attention_mask = torch.tensor([1] * len(processed_tokens))\n",
        "\n",
        "        outputs = (input_ids, token_type_ids, attention_mask)\n",
        "\n",
        "        if self.for_train:\n",
        "            label = self.labels[idx]\n",
        "            label = torch.tensor(label)\n",
        "            outputs += (label, )\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def create_mini_batch(self, samples):\n",
        "        outputs = list(zip(*samples))\n",
        "\n",
        "        # zero pad 到同一序列長度\n",
        "        input_ids = pad_sequence(outputs[0], batch_first=True)\n",
        "        token_type_ids = pad_sequence(outputs[1], batch_first=True)\n",
        "        attention_mask = pad_sequence(outputs[2], batch_first=True)\n",
        "\n",
        "        batch_output = (input_ids, token_type_ids, attention_mask)\n",
        "    \n",
        "        if self.for_train:\n",
        "            labels = torch.stack(outputs[3])\n",
        "            batch_output += (labels, )\n",
        "\n",
        "        return batch_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OkQ7x2jYAcI"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "dataset = MultiClassDataset(tokenizer, df)\n",
        "\n",
        "CUT_RATIO = 0.8\n",
        "train_size = int(CUT_RATIO * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_8COlqIYMl5"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=dataset.create_mini_batch,\n",
        "    shuffle=True\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=dataset.create_mini_batch,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyWshgkTZn5V"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7g4EF2lYkEZ",
        "outputId": "eb49807b-f739-403e-c9d4-79d479cd4914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'device: {device}')\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, \n",
        "    num_labels = 2,\n",
        "    return_dict=True\n",
        ")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCGxfV6UaLx4"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFYQNV_eaaVA"
      },
      "source": [
        "def train_batch(model, data, optimizer, device):\n",
        "    model.train()\n",
        "    input_ids, token_type_ids, attention_mask, labels = [d.to(device) for d in data]\n",
        "\n",
        "    outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        token_type_ids=token_type_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        labels=labels\n",
        "    )\n",
        "    loss = outputs.loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, valid_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss = RunningAverage()\n",
        "    acc = RunningAverage()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(valid_loader, desc='evaluate'):\n",
        "            input_ids, token_type_ids, attention_mask, labels = [d.to(device) for d in data]\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                token_type_ids=token_type_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss.add(outputs.loss.item())\n",
        "            corrects = (outputs.logits.argmax(dim=-1) == labels).cpu().tolist()\n",
        "            acc.add_all(corrects)\n",
        "\n",
        "    return loss.get(), acc.get()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZp1-O0KaJ-r",
        "outputId": "e1d3108f-89ab-43ff-99d1-45a5b4ef617b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lr = 0.00001\n",
        "max_iter = 200\n",
        "show_per_iter = 10\n",
        "valid_per_iter = 50\n",
        "save_per_iter = 100\n",
        "save_checkpoint_dir = 'models/'\n",
        "model_prefix = 'cn_sentiment_class_'\n",
        "\n",
        "assert save_per_iter % valid_per_iter == 0\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "i = 1\n",
        "is_running = True\n",
        "train_loss = RunningAverage()\n",
        "model_paths = []\n",
        "while is_running:\n",
        "    for train_data in train_loader:\n",
        "        loss = train_batch(model, train_data, optimizer, device)\n",
        "        train_loss.add(loss)\n",
        "\n",
        "        if i % show_per_iter == 0:\n",
        "            print('train [{}]: loss={}'.format(i, train_loss.get()))\n",
        "            train_loss.flush()\n",
        "\n",
        "        if i % valid_per_iter == 0:\n",
        "            loss, acc = evaluate(model, valid_loader, device)\n",
        "            print(f'valid: loss={loss}, acc={acc}')\n",
        "\n",
        "        if i % save_per_iter == 0:\n",
        "            path = os.path.join(save_checkpoint_dir, model_prefix + f'loss{loss:.5}/')\n",
        "            print(f'save model at {path}')\n",
        "            model.save_pretrained(path)\n",
        "            model_paths.append(path)\n",
        "        \n",
        "        if i == max_iter:\n",
        "            is_running = False\n",
        "            break\n",
        "\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train [10]: loss=0.7181851685047149\n",
            "train [20]: loss=0.5864066839218139\n",
            "train [30]: loss=0.42693847715854644\n",
            "train [40]: loss=0.24774658530950547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate:   0%|          | 1/438 [00:00<01:12,  6.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train [50]: loss=0.16719756051898002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate: 100%|██████████| 438/438 [01:12<00:00,  6.02it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid: loss=0.15826657107397574, acc=0.9472142857142857\n",
            "train [60]: loss=0.20298943296074867\n",
            "train [70]: loss=0.1366733867675066\n",
            "train [80]: loss=0.13427007235586644\n",
            "train [90]: loss=0.1230307761579752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate:   0%|          | 1/438 [00:00<01:23,  5.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train [100]: loss=0.11760147921741008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate: 100%|██████████| 438/438 [01:15<00:00,  5.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid: loss=0.1247269513200305, acc=0.9582857142857143\n",
            "save model at models/cn_sentiment_class_loss0.12473/\n",
            "train [110]: loss=0.1479931315407157\n",
            "train [120]: loss=0.16510998792946338\n",
            "train [130]: loss=0.17600416839122773\n",
            "train [140]: loss=0.13213363960385321\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate:   0%|          | 1/438 [00:00<01:18,  5.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train [150]: loss=0.09483904615044594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate: 100%|██████████| 438/438 [01:17<00:00,  5.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid: loss=0.10906191855949694, acc=0.9624285714285714\n",
            "train [160]: loss=0.11372225657105446\n",
            "train [170]: loss=0.08925026133656502\n",
            "train [180]: loss=0.11515358416363597\n",
            "train [190]: loss=0.11444938695058227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate:   0%|          | 1/438 [00:00<01:20,  5.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train [200]: loss=0.13236291687935592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "evaluate: 100%|██████████| 438/438 [01:16<00:00,  5.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid: loss=0.0945605286737249, acc=0.9663571428571428\n",
            "save model at models/cn_sentiment_class_loss0.094561/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3oe44pdrOrk"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBgxvvHybnFb",
        "outputId": "651758a3-a41b-4313-e124-453cbb5fe61b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "reload_checkpoint = model_paths[-1]\n",
        "\n",
        "examples = [\n",
        "    '板金的部分我覺得很脆弱',\n",
        "    '整體外殼造型我是喜歡的'\n",
        "]\n",
        "examples_df = pd.DataFrame(data={'text': examples})\n",
        "\n",
        "pred_dataset = MultiClassDataset(tokenizer, examples_df, for_train=False)\n",
        "\n",
        "pred_loader = DataLoader(\n",
        "    dataset=pred_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=pred_dataset.create_mini_batch,\n",
        ")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(reload_checkpoint)\n",
        "model.to(device)\n",
        "\n",
        "pred_labels = []\n",
        "with torch.no_grad():\n",
        "    for data in tqdm(pred_loader, desc='predict'):\n",
        "        input_ids, token_type_ids, attention_mask = [d.to(device) for d in data]\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        pred_labels += outputs.logits.argmax(dim=-1).cpu().tolist()\n",
        "\n",
        "print('predict result: ', list(zip(examples, pred_labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict: 100%|██████████| 1/1 [00:00<00:00, 59.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predict result:  [('板金的部分我覺得很脆弱', 0), ('整體外殼造型我是喜歡的', 1)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}